
import urllib.request
import bs4 as bs

#extract from sitemaps from a news website
#constantly track sitemaps for bots

#Websites will not give you sitemaps if you are doing it programmatically
#so we create a user agent to fake the websites
headers={}

headers['User-Agent']='Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17'

url='http://www.dailynews.com/section?template=RSS&profile=4000036&mime=xml'

req = urllib.request.Request(url,headers=headers)

sauce=urllib.request.urlopen(req).read()

soup=bs.BeautifulSoup(sauce,'xml')


distinct_item=[]

for item in soup.find_all('item'):
    distinct_item.append(item)


class news:

    def __init__(self,url_list):

        for url in url_list:
            self.guid= url.find('guid').text
            self.link=url.find('link').text

            self.description=url.find('description').text
            self.pub_date=url.find('pubDate').text
            self.title=url.find('title').text
            self.update_date=url.find('updateDate').text
            self.override_url=url.find('overrideUrl').text
            self.category=[category.text for category in url.find_all('category')]




    def get_categories(self):

        print(self.category)


    def get_title(self, item):

        title= item.find('title').text
        return title

    def get_description(self,item):

        desc= item.find('description').text

        return desc















